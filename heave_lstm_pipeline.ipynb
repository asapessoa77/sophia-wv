{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Heave → LSTM → FFT (fase zero) → Zero-Downcrossing\n",
        "\n",
        "Notebook **análogo** aos anteriores, agora com **LSTM (Keras/TensorFlow)** para previsão de curto prazo da série de **heave**.\n",
        "\n",
        "## Objetivo\n",
        "- Prever **heave** com uma **LSTM** de baixa complexidade (seq→seq multi-passos);\n",
        "- Aplicar **FFT** para **regularização espectral** (passa-faixa, fase zero);\n",
        "- Derivar **N, Hs, Hmean, Hmax, Tz** via **zero-downcrossing**;\n",
        "- Avaliar vs. **Persistência** e (opcionalmente) vs. ARIMA/ETS, usando *rolling origin* (+30, +60, +120 passos).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dependências\n",
        "`pandas`, `numpy`, `matplotlib`, `scipy`, `tensorflow` (>=2.x), `sklearn`.\n",
        "\n",
        "```bash\n",
        "pip install pandas numpy matplotlib scipy scikit-learn tensorflow\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-21 11:28:29.701110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-10-21 11:28:29.712955: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-10-21 11:28:29.717179: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-10-21 11:28:29.726783: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-10-21 11:28:30.528300: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import welch\n",
        "from numpy.fft import rfft, irfft, rfftfreq\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "plt.rcParams['figure.figsize']=(10,4)\n",
        "pd.options.display.float_format = '{:.6f}'.format\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuração & Leitura de Dados\n",
        "Esperado um CSV com colunas `timestamp` e `heave`. Ajuste caminhos e hiperparâmetros conforme sua série."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# === CONFIG ===\n",
        "CSV_PATH = './dados/out/heave_6613_concat.csv'\n",
        "TIME_COL = 'time'\n",
        "VAL_COL  = 'Upward'\n",
        "FREQ     = None   # ex.: 'T' se minutely; None para inferir\n",
        "\n",
        "# Horizontes (número de passos do sampling)\n",
        "HORIZONS = [30, 60, 120]\n",
        "\n",
        "# Janela de entrada (lookback) e estratégia seq→seq (prevê H passos)\n",
        "LOOKBACK = 256\n",
        "EPOCHS   = 20\n",
        "BATCH    = 64\n",
        "LR       = 1e-3\n",
        "PATIENCE = 3\n",
        "\n",
        "# FFT bandpass (Hz) – ajuste após inspecionar o espectro (célula PSD)\n",
        "F_MIN_HZ = 1/1000\n",
        "F_MAX_HZ = 1/4\n",
        "\n",
        "# Welch params\n",
        "WELCH_WINDOW='hann'; WELCH_SEG=256; WELCH_OVERLAP=0.5\n",
        "\n",
        "# Zero-cross limiar\n",
        "HMIN = 0.0\n",
        "\n",
        "# df = pd.read_csv(CSV_PATH)\n",
        "# df[TIME_COL] = pd.to_datetime(df[TIME_COL])\n",
        "# df = df.sort_values(TIME_COL).set_index(TIME_COL)\n",
        "# df = df[[VAL_COL]].dropna()\n",
        "# df = df.asfreq(FREQ or pd.infer_freq(df.index))\n",
        "# fs = 1.0/(df.index.to_series().diff().median().total_seconds())\n",
        "# print('fs (Hz):', fs)\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c318be5c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regra de reamostragem: 160ms | fs (Hz): 6.25\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Upward</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>time</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2025-07-14 00:00:00.000</th>\n",
              "      <td>0.030000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-07-14 00:00:00.160</th>\n",
              "      <td>-0.060000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-07-14 00:00:00.320</th>\n",
              "      <td>-0.120000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-07-14 00:00:00.480</th>\n",
              "      <td>-0.140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-07-14 00:00:00.640</th>\n",
              "      <td>-0.120000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           Upward\n",
              "time                             \n",
              "2025-07-14 00:00:00.000  0.030000\n",
              "2025-07-14 00:00:00.160 -0.060000\n",
              "2025-07-14 00:00:00.320 -0.120000\n",
              "2025-07-14 00:00:00.480 -0.140000\n",
              "2025-07-14 00:00:00.640 -0.120000"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(CSV_PATH)\n",
        "df[TIME_COL] = pd.to_datetime(df[TIME_COL], utc=False, errors='coerce')\n",
        "df = df.dropna(subset=[TIME_COL, VAL_COL]).sort_values(TIME_COL).set_index(TIME_COL)\n",
        "df = df[[VAL_COL]].astype(float)\n",
        "\n",
        "# 1) tratar duplicatas (escolha UMA das políticas abaixo):\n",
        "\n",
        "# (a) manter a primeira ocorrência\n",
        "# df = df[~df.index.duplicated(keep='first')]\n",
        "\n",
        "# (b) ou agregar duplicatas por média (recomendado para sensores)\n",
        "df = df.groupby(level=0).mean()\n",
        "\n",
        "# 2) inferir passo temporal robusto (mediana dos deltas)\n",
        "dt = df.index.to_series().diff().dropna()\n",
        "# se tiver timezone, remova ou padronize antes\n",
        "step_seconds = dt.dt.total_seconds().median()\n",
        "\n",
        "# opcional: se você já souber a frequência, force aqui:\n",
        "if FREQ is not None:\n",
        "    rule = FREQ  # ex.: 'S' segundo, 'T' minuto, '500L' 500 ms\n",
        "else:\n",
        "    # cria uma regra a partir do passo mediano\n",
        "    # arredonda para as unidades usuais (s, ms) se fizer sentido\n",
        "    from pandas.tseries.frequencies import to_offset\n",
        "    rule = to_offset(pd.Timedelta(seconds=step_seconds)).freqstr\n",
        "\n",
        "# 3) reamostrar para uma grade regular (evita problemas do asfreq com jitter)\n",
        "# use .mean() (ou .nearest()) e depois complete lacunas com interpolação temporal\n",
        "df = df.resample(rule).mean()\n",
        "\n",
        "# preencher pequenas lacunas de forma segura (ajuste o limite conforme seu caso)\n",
        "df[VAL_COL] = df[VAL_COL].interpolate(method='time', limit=5)  # até 5 intervalos seguidos\n",
        "\n",
        "# 4) frequência de amostragem (Hz) a partir da grade regular\n",
        "if len(df.index) >= 2:\n",
        "    fs = 1.0 / (df.index[1] - df.index[0]).total_seconds()\n",
        "else:\n",
        "    raise ValueError(\"Série muito curta após o pré-processamento.\")\n",
        "\n",
        "print('Regra de reamostragem:', rule, '| fs (Hz):', fs)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utilitários (detrend, Welch/FFT, filtro fase-zero, zero-downcrossing, métricas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def detrend_mean(x):\n",
        "    x = np.asarray(x)\n",
        "    return x - np.nanmean(x)\n",
        "\n",
        "def psd_welch(x, fs, nperseg=256, noverlap=0.5, window='hann'):\n",
        "    n_ov = int(nperseg*noverlap)\n",
        "    f,Pxx = welch(x, fs=fs, window=window, nperseg=nperseg, noverlap=n_ov, detrend=False)\n",
        "    return f,Pxx\n",
        "\n",
        "def bandpass_fft_phase_zero(x, fs, fmin, fmax):\n",
        "    x = detrend_mean(x)\n",
        "    N = len(x)\n",
        "    X = rfft(x)\n",
        "    freqs = rfftfreq(N, d=1/fs)\n",
        "    mask = (freqs>=fmin)&(freqs<=fmax)\n",
        "    Xf = X*mask\n",
        "    return irfft(Xf, n=N)\n",
        "\n",
        "def zero_downcrossing(h, fs, hmin=0.0):\n",
        "    h = np.asarray(h)\n",
        "    N = len(h)\n",
        "    idx = np.where((h[:-1]>0)&(h[1:]<=0))[0]\n",
        "    if len(idx)<2:\n",
        "        return dict(N=0,Hmean=np.nan,Hmax=np.nan,Hs=np.nan,Tz=np.nan)\n",
        "    t = np.arange(N)/fs\n",
        "    t_cross = t[idx] + (h[idx]/(h[idx]-h[idx+1]))*(1/fs)\n",
        "    H_list=[]; T_list=[]\n",
        "    for k in range(len(t_cross)-1):\n",
        "        t0,t1 = t_cross[k], t_cross[k+1]\n",
        "        i0,i1 = int(np.floor(t0*fs)), int(np.ceil(t1*fs))\n",
        "        if i1<=i0+1: continue\n",
        "        seg = h[i0:i1+1]\n",
        "        crest = np.max(seg); trough = np.min(seg)\n",
        "        H = crest - trough\n",
        "        if H < hmin: continue\n",
        "        H_list.append(H); T_list.append(t1-t0)\n",
        "    if not H_list:\n",
        "        return dict(N=0,Hmean=np.nan,Hmax=np.nan,Hs=np.nan,Tz=np.nan)\n",
        "    H_arr = np.array(H_list); T_arr=np.array(T_list)\n",
        "    Hmean = H_arr.mean(); Hmax=H_arr.max()\n",
        "    top_n = max(1,int(np.ceil(len(H_arr)/3)))\n",
        "    Hs = np.sort(H_arr)[::-1][:top_n].mean()\n",
        "    Tz = T_arr.mean()\n",
        "    return dict(N=len(H_arr), Hmean=Hmean, Hmax=Hmax, Hs=Hs, Tz=Tz)\n",
        "\n",
        "def metrics(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true); y_pred=np.asarray(y_pred)\n",
        "    e = y_pred - y_true\n",
        "    rmse = float(np.sqrt(np.mean(e**2)))\n",
        "    mae  = float(np.mean(np.abs(e)))\n",
        "    smape = float(np.mean(2*np.abs(e)/np.where((np.abs(y_true)+np.abs(y_pred))==0,1,(np.abs(y_true)+np.abs(y_pred)))))\n",
        "    return dict(RMSE=rmse, MAE=mae, sMAPE=smape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparação de dados para LSTM (janelas seq→seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def make_windows(y: np.ndarray, lookback: int, horizon: int):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(y)-lookback-horizon+1):\n",
        "        X.append(y[i:i+lookback])\n",
        "        Y.append(y[i+lookback:i+lookback+horizon])\n",
        "    X = np.array(X)[:, :, None]  # (N, lookback, 1)\n",
        "    Y = np.array(Y)              # (N, horizon)\n",
        "    return X, Y\n",
        "\n",
        "def build_lstm(lookback: int, horizon: int, lr: float=1e-3):\n",
        "    inp = layers.Input(shape=(lookback,1))\n",
        "    x = layers.LSTM(64, return_sequences=True)(inp)\n",
        "    x = layers.LSTM(32)(x)\n",
        "    out = layers.Dense(horizon)(x)\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(optimizer=optimizers.Adam(lr), loss='mse', metrics=['mae'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rolling Origin com LSTM\n",
        "Treina uma LSTM por corte (treino curto) e prevê **H** passos; aplica FFT fase-zero e zero-downcrossing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def rolling_origin_lstm(df, val_col, horizons, fs,\n",
        "                        lookback=256, epochs=20, batch=64, lr=1e-3, patience=3,\n",
        "                        train_min_points=1500,\n",
        "                        fmin=1/1000, fmax=1/4, hmin=0.0):\n",
        "    y = df[val_col].astype(float).values\n",
        "    scaler = StandardScaler()\n",
        "    res_rows=[]\n",
        "    for cut in range(train_min_points, len(y)-max(horizons)-1):\n",
        "        y_train = y[:cut]\n",
        "        for H in horizons:\n",
        "            y_true = y[cut:cut+H]\n",
        "            # normaliza por treino\n",
        "            y_train_sc = scaler.fit_transform(y_train.reshape(-1,1)).ravel()\n",
        "            Xtr, Ytr = make_windows(y_train_sc, lookback, H)\n",
        "            if len(Xtr)==0:\n",
        "                continue\n",
        "            model = build_lstm(lookback, H, lr)\n",
        "            es = callbacks.EarlyStopping(patience=patience, restore_best_weights=True)\n",
        "            model.fit(Xtr, Ytr, epochs=epochs, batch_size=batch, verbose=0, validation_split=0.1, callbacks=[es])\n",
        "\n",
        "            # entrada: última janela do treino (normalizada)\n",
        "            x_last = y_train_sc[-lookback:][None,:,None]\n",
        "            y_pred_sc = model.predict(x_last, verbose=0)[0]\n",
        "            # desscala previsão\n",
        "            y_pred = scaler.inverse_transform(y_pred_sc.reshape(-1,1)).ravel()\n",
        "\n",
        "            # baseline persistência\n",
        "            y_pers = np.repeat(y_train[-1], H)\n",
        "\n",
        "            # filtragem FFT fase-zero\n",
        "            y_true_f = bandpass_fft_phase_zero(y_true, fs, fmin, fmax)\n",
        "            y_pred_f = bandpass_fft_phase_zero(y_pred, fs, fmin, fmax)\n",
        "            y_pers_f = bandpass_fft_phase_zero(y_pers, fs, fmin, fmax)\n",
        "\n",
        "            # métricas heave\n",
        "            m_lstm = metrics(y_true, y_pred)\n",
        "            m_pers = metrics(y_true, y_pers)\n",
        "\n",
        "            # parâmetros\n",
        "            w_true = zero_downcrossing(y_true_f, fs, hmin)\n",
        "            w_lstm = zero_downcrossing(y_pred_f, fs, hmin)\n",
        "            w_pers = zero_downcrossing(y_pers_f, fs, hmin)\n",
        "\n",
        "            for name, m, w in [('LSTM', m_lstm, w_lstm), ('Persistence', m_pers, w_pers)]:\n",
        "                res_rows.append({\n",
        "                    'cut_idx':cut, 'horizon':H, 'model':name,\n",
        "                    'RMSE':m['RMSE'], 'MAE':m['MAE'], 'sMAPE':m['sMAPE'],\n",
        "                    'N_true':w_true.get('N'), 'Hmean_true':w_true.get('Hmean'), 'Hmax_true':w_true.get('Hmax'), 'Hs_true':w_true.get('Hs'), 'Tz_true':w_true.get('Tz'),\n",
        "                    'N_pred':w.get('N'), 'Hmean_pred':w.get('Hmean'), 'Hmax_pred':w.get('Hmax'), 'Hs_pred':w.get('Hs'), 'Tz_pred':w.get('Tz'),\n",
        "                })\n",
        "    return pd.DataFrame(res_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rodar (ajuste `train_min_points`, `LOOKBACK`, `EPOCHS` conforme seu dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df_res_lstm = rolling_origin_lstm(\n",
        "    df, VAL_COL, HORIZONS, fs,\n",
        "    lookback=LOOKBACK, epochs=EPOCHS, batch=BATCH, lr=LR, patience=PATIENCE,\n",
        "    train_min_points=1500,\n",
        "    fmin=F_MIN_HZ, fmax=F_MAX_HZ, hmin=HMIN\n",
        ")\n",
        "df_res_lstm.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sumário de métricas (heave e parâmetros)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def relerr(true, pred):\n",
        "    return 100.0*(np.asarray(pred)-np.asarray(true))/np.where(np.asarray(true)==0, np.nan, np.asarray(true))\n",
        "\n",
        "def summarize(df_res):\n",
        "    out=[]\n",
        "    for (H, model), g in df_res.groupby(['horizon','model']):\n",
        "        hs_rel = relerr(g['Hs_true'], g['Hs_pred'])\n",
        "        tz_rel = relerr(g['Tz_true'], g['Tz_pred'])\n",
        "        out.append({'horizon':H,'model':model,\n",
        "                    'RMSE':g['RMSE'].mean(), 'MAE':g['MAE'].mean(), 'sMAPE':g['sMAPE'].mean(),\n",
        "                    'Hs_relerr_pct_mean':np.nanmean(hs_rel), 'Tz_relerr_pct_mean':np.nanmean(tz_rel)})\n",
        "    return pd.DataFrame(out)\n",
        "\n",
        "summary_lstm = summarize(df_res_lstm)\n",
        "summary_lstm.sort_values(['horizon','model'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PSD (Welch) para ajuste de banda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "x = df[VAL_COL].astype(float).values\n",
        "x_d = detrend_mean(x)\n",
        "f_psd, Pxx = psd_welch(x_d, fs, nperseg=WELCH_SEG, noverlap=WELCH_OVERLAP, window=WELCH_WINDOW)\n",
        "plt.plot(f_psd, Pxx)\n",
        "plt.xlabel('Frequência (Hz)'); plt.ylabel('PSD'); plt.title('Welch PSD - Heave (detrended)'); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exportar resultados (CSV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "outdir = Path('./outputs_lstm')\n",
        "outdir.mkdir(parents=True, exist_ok=True)\n",
        "df_res_lstm.to_csv(outdir/'rolling_results_lstm.csv', index=False)\n",
        "summary_lstm.to_csv(outdir/'summary_metrics_lstm.csv', index=False)\n",
        "print('Arquivos salvos em', outdir.resolve())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notas\n",
        "- A LSTM é **leve** (64→32 unidades). Ajuste LOOKBACK, camadas e regularização para seu dataset.\n",
        "- Use **EarlyStopping** para evitar *overfitting*; reduza EPOCHS se for rodar em tempo de operação.\n",
        "- A normalização é **reajustada** em cada corte usando apenas dados de treino (evita *leakage*).\n",
        "- Mantemos **fase zero** na filtragem por máscara simétrica em FFT antes do zero-crossing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Referências (ABNT)\n",
        "- HOLTHUIJSEN, L. H. **Waves in Oceanic and Coastal Waters**. Cambridge University Press, 2007.\n",
        "- GODA, Y. **Random Seas and Design of Maritime Structures**. World Scientific, 2010.\n",
        "- OPPENHEIM, A. V.; SCHAFER, R. W. **Discrete-Time Signal Processing**. Prentice Hall, 2010.\n",
        "- GOODFELLOW, I.; BENGIO, Y.; COURVILLE, A. **Deep Learning**. MIT Press, 2016 (cap. RNNs).\n",
        "- HYNDMAN, R. J.; ATHANASOPOULOS, G. **Forecasting: Principles and Practice** (cap. redes neurais para previsão), 2021.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
